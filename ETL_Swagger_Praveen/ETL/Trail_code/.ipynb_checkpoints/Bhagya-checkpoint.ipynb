{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import os, time  #import statements\n",
    "from datetime import datetime\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from stat import *\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/local/bin/python3\"\n",
    "startTime = datetime.now()\n",
    "\n",
    "import datetime\n",
    "ts = time.time()\n",
    "currentTimeStamp = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "logging.basicConfig(filename='Ingestion_'+currentTimeStamp+'.log', filemode='w',format='%(asctime)s - %(message)s',datefmt='%d-%b-%y %H:%M:%S', level=logging.INFO)\n",
    "logging.info('Job Started : %s',currentTimeStamp)\n",
    "SparkContext.setSystemProperty(\"hive.metastore.uris\", \"thrift://10.3.2.20:9083\")\n",
    "sparkSession = (SparkSession.builder.appName('pyspark-to-load-tables-hive').enableHiveSupport().getOrCreate())\n",
    "spark = SparkSession.builder.appName('changeColNames').getOrCreate()\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "logging.info('User logged in and created SparkContext and SparkSession')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedname = 'Churn Prediction'\n",
    "jsonpath = \"/home/etl/ETL/mysqlDetails.json\"\n",
    "data = pd.read_json(jsonpath,typ='series',orient='columns')\n",
    "DB_name= data.DB_name\n",
    "user=data.user\n",
    "password=data.password\n",
    "host=data.host\n",
    "feedControlquery= 'SELECT * FROM etl.feedcontrol WHERE feedname' + \" = '\" + feedname + \"'\"\n",
    "\n",
    "conn = mysql.connector.connect(\n",
    "     host=host,\n",
    "     database=DB_name,\n",
    "     user=user,\n",
    "     password=password) \n",
    "feedcontrol = pd.read_sql(feedControlquery, conn)\n",
    "feed = spark.createDataFrame(feedcontrol)\n",
    "\n",
    "calenderquery = 'SELECT * FROM etl.calender where openindicator =\"Y\" '\n",
    "calender_pd = pd.read_sql(calenderquery, conn)\n",
    "calender = spark.createDataFrame(calender_pd)\n",
    "\n",
    "filemetadataquery = 'SELECT * FROM etl.filemetadata' + ' WHERE feedname' + \" ='\" + feedname + \"'\"\n",
    "filemetadata_pd = pd.read_sql(filemetadataquery, conn)\n",
    "filemetadata = spark.createDataFrame(filemetadata_pd)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|   busdate|openindicator|\n",
      "+----------+-------------+\n",
      "|2019-05-29|            Y|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calender.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "9\n",
      "Churn Prediction\n",
      "WA_Fn-UseC_-Telco-Customer-Chu\n",
      "/home/etl/ETL/inputdata\n",
      "telcocustomerchurn\n",
      "N\n",
      "DELIMITED\n",
      "hdfs://10.3.2.13:8020/user/had\n",
      "passthrough\n",
      "2019-05-29\n",
      "/home/etl/ETL/inputdata/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
      "rawpath:hdfs://10.3.2.13:8020/user/hadoop/rawzone/WA_Fn-UseC_-Telco-Customer-Churn.csv \n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    df_ATracker = sparkSession.sql('SELECT max(trackingid) as attendence_id FROM ETL.Attendence_Tracker order by attendence_id ')  #Attendence_tracker\n",
    "    if(df_ATracker.collect()[0]['attendence_id']== None):\n",
    "       attendence_id = 1\n",
    "    else:\n",
    "       attendence_id = int(df_ATracker.collect()[0]['attendence_id']) \n",
    "       attendence_id+=1 \n",
    "    logging.info('value of attendence_id is %s', attendence_id)\n",
    "    print(df_ATracker.collect()[0]['attendence_id'])\n",
    "    print(attendence_id)\n",
    "except Exception as e:\n",
    "    logging.error('Error occured while creating dataframe for attendence_tarcker tables', exc_info=True)\n",
    "feedname = feed.collect()[0]['feedname']\n",
    "print(feedname)\n",
    "filename = feed.collect()[0]['filename']\n",
    "print(filename)\n",
    "landingpath = feed.collect()[0]['landingpath']\n",
    "print(landingpath)\n",
    "sourceTableName = feed.collect()[0]['sourcetablename']\n",
    "print(sourceTableName)\n",
    "header_trailer_flag = feed.collect()[0]['headertrailerflag']\n",
    "print(header_trailer_flag)\n",
    "fileformat = feed.collect()[0]['dataformat']\n",
    "print(fileformat)\n",
    "rawzonepath = feed.collect()[0]['rawzonepath']\n",
    "print(rawzonepath)\n",
    "processType = feed.collect()[0]['processtype']\n",
    "print(processType)\n",
    "busdate_calender = calender.collect()[0]['busdate']\n",
    "print(busdate_calender)\n",
    "if(fileformat == 'DELIMITED' or fileformat == 'FIXED'):\n",
    "    filedelimiter = filemetadata.collect()[0]['filedelimiter']\n",
    "    path = '/home/etl/ETL/inputdata/WA_Fn-UseC_-Telco-Customer-Churn.csv' #str(\"%s/%s\" % (landingpath, filename))\n",
    "    print(path)\n",
    "else:\n",
    "    path = str(\"%s/%s%s%s\" % (landingpath, filename,\".\",fileformat))\n",
    "    print(path)\n",
    "if(fileformat == 'database'):\n",
    "    jsonpath = \"/home/etl/ETL/mysqlDetails.json\"\n",
    "    ArrivalTimeStamp=currentTimeStamp\n",
    "    FileSize = 0\n",
    "    logging.info('ArrivalTimestamp for MysqlDB fileformat %s',ArrivalTimeStamp)\n",
    "else:     #extracting arrival time stamp of file from system\n",
    "    ArrivalTimeStamp = time.ctime(os.path.getctime(path))\n",
    "    st = os.stat(path)\n",
    "    FileSize = st[ST_SIZE]\n",
    "rawzonepath = 'hdfs://10.3.2.13:8020/user/hadoop/rawzone/WA_Fn-UseC_-Telco-Customer-Churn.csv' #str(\"%s%s\" % (rawzonepath, filename))\n",
    "print(\"rawpath:%s \" %rawzonepath)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WithHeaderTrailer(attendence_id,feedname, filename, path, filedelimiter, ArrivalTimeStamp, FileSize, busdateposition, numofrowspos, busdate_calender,processType,fileformat,headerIdentifier,detailIdentifier,trailerIdentifier):\n",
    "        logging.info('Job is in function WithHeaderTrailer')\n",
    "        lines = sc.textFile(path)\n",
    "        H = lines.filter(lambda l: l.startswith(headerIdentifier)) \n",
    "        H.collect()\n",
    "        header = H.take(1)  #extracting header data\n",
    "        Header = ''.join(header)    \n",
    "        Date = H.map(lambda l: l.split(filedelimiter)[busdateposition-1]) #extracting business date\n",
    "        DateOfExtract = Date.collect().pop(0)\n",
    "        Detail = lines.filter(lambda l: l.startswith(detailIdentifier))  #separating detail data\n",
    "        Detail.collect()\n",
    "        Detail_Count = Detail.count() #counting no of rows\n",
    "        print(Detail_Count)\n",
    "        T = lines.filter(lambda l: l.startswith(trailerIdentifier))  #separating trailer data\n",
    "        T.collect()\n",
    "        trailer = T.take(1)  #extracting trailer data\n",
    "        Trailer = ''.join(trailer)\n",
    "        NOR = T.map(lambda l: l.split(filedelimiter)[numofrowspos-1]) #extracting date of number of rows\n",
    "        NoOfRecords = int(NOR.collect().pop(0))\n",
    "        print(NoOfRecords) #validating date of extract\n",
    "        if DateOfExtract == busdate_calender:\n",
    "            HeaderVldFlag = 'Y'\n",
    "            ErrorCodeList = 'No Error'\n",
    "        else:\n",
    "            HeaderVldFlag = 'N'\n",
    "            ErrorCodeList = 'Invalid Busdate'\n",
    "        if NoOfRecords == Detail_Count: \n",
    "    #validating number of rows\n",
    "            TrailerVldFlag = 'Y'\n",
    "            ErrorCodeList = 'No Error'\n",
    "        else:\n",
    "            TrailerVldFlag = 'N'\n",
    "            ErrorCodeList = 'Invalid NoOfRecords'          \n",
    "        \n",
    "        \n",
    "        dataframe = spark.createDataFrame(Detail, StringType())\n",
    "        data = dataframe.replace(\"\\\"\",\"\")                   #.map( lambda elem: elem.strip('\\\"'))\n",
    "        data.show()\n",
    "        data.write.format('csv').save('/home/etl/ETL/dataframecheck')\n",
    "        #Ingestion Into Rawzone\n",
    "        #rawzonelocation = IngestionIntoRawzone(attendence_id,processType,fileformat,DateOfExtract,dataframe)\n",
    "        #attendence tracker table\n",
    "        #print(rawzonelocation)\n",
    "        #Attendence_tracking(attendence_id, feedname, filename, path, ArrivalTimeStamp, currentTimeStamp,FileSize, DateOfExtract,rawzonelocation, Header, Trailer, HeaderVldFlag, TrailerVldFlag, ErrorCodeList)\n",
    "        #logging.info('Data has been entered into Attendence_tracker')\n",
    "            \n",
    "            \n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WithOutHeaderTrailer(attendence_id,feedname, filename, path,filedelimiter, ArrivalTimeStamp, FileSize,busdate_calender,processType):\n",
    "        logging.info('Job is in function WithOutHeaderTrailer')\n",
    "        dataframe = spark.read.load('/home/etl/ETL/inputdata/WA_Fn-UseC_-Telco-Customer-Churn.csv', format='csv', sep=filedelimiter)\n",
    "        #Ingestion Into Rawzone\n",
    "        rawzonelocation = IngestionIntoRawzone(attendence_id,processType,fileformat,busdate_calender,dataframe)\n",
    "        #attendence tracker table\n",
    "        logging.info(rawzonelocation)\n",
    "        Attendence_tracking(attendence_id,feedname, filename, path, ArrivalTimeStamp,currentTimeStamp, FileSize, busdate_calender,rawzonelocation,\"null\",\"null\",\"null\",\"null\",\"null\")\n",
    "        logging.info('Data has been entered into Attendence_tracker')\n",
    "            \n",
    "            \n",
    "         \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MySqlDB(attendence_id,feedname, filename, jsonpath,processType,busdate_calender):\n",
    "    logging.info('Job is in function MySqlDB')\n",
    "    data = pd.read_json(jsonpath,typ='series',orient='columns')\n",
    "    DB_name= data.DB_name\n",
    "    user=data.user\n",
    "    password=data.password\n",
    "    host=data.host\n",
    "    query=data.query\n",
    "    conn = mysql.connector.connect(\n",
    "         host=host,\n",
    "         database=DB_name,\n",
    "         user=user,\n",
    "         password=password)\n",
    "    #attendence tracker table \n",
    "    pd_df = pd.read_sql(query, conn)\n",
    "    dataframe = spark.createDataFrame(pd_df)\n",
    "    dataframe.show()\n",
    "    rawzonelocation = IngestionIntoRawzone(attendence_id,processType,fileformat,busdate_calender,dataframe)\n",
    "    print(rawzonelocation)\n",
    "    Attendence_tracking(attendence_id,feedname, filename, \"\", ArrivalTimeStamp,currentTimeStamp, FileSize, busdate_calender,rawzonelocation,\"\",\"\",\"\",\"\",\"\")    \n",
    "        \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attendence_tracking(attendence_id, feedname, filename, path, ArrivalTimeStamp, currentTimeStamp,FileSize, DateOfExtract,rawzonepath, Header, Trailer, HeaderVldFlag, TrailerVldFlag, ErrorCodeList):\n",
    "    sparkSession.sql(\"insert into table etl.attendence_tracker select '{}','{}','{}','{}','{}','{}','{}','{}','{}','{}','{}','{}','{}','{}'\".format(attendence_id, feedname, filename, path, ArrivalTimeStamp, currentTimeStamp,FileSize, DateOfExtract,rawzonepath, Header, Trailer, HeaderVldFlag, TrailerVldFlag, ErrorCodeList))\n",
    "\n",
    "def IngestionIntoRawzone(attendence_id,processType,fileformat,busdate_calender,dataframe):\n",
    "    try:\n",
    "        dataframe = dataframe.withColumn('attendence_id',f.lit(attendence_id))\n",
    "        #Date_date=datetime.datetime.strptime(busdate_calender, '%d/%m/%Y')\n",
    "        #print(busdate_calender)\n",
    "        Date=busdate_calender\n",
    "        print(Date)\n",
    "        df = dataframe.withColumn('BusDate',f.lit(Date))\n",
    "        if(fileformat == 'DELIMITED'):\n",
    "            df.write.format('csv').partitionBy('BusDate','attendence_id').option('delimiter', filedelimiter).save(rawzonepath, mode='append')\n",
    "            partitionAlterSql = 'ALTER TABLE etl.' + sourceTableName + ' ADD IF NOT EXISTS PARTITION (' + 'BusDate' + \"='\" + str(Date) + \"',\" + 'attendence_id' + \"=\" + str(attendence_id) +\") LOCATION '\" + rawzonepath + '/' + 'BusDate' + \"=\" + str(Date) + '/' + 'attendence_id' + \"=\" + str(attendence_id) +\"'\"\n",
    "            logging.info(partitionAlterSql)\n",
    "            sparkSession.sql(partitionAlterSql)\n",
    "        elif(fileformat == 'database'):\n",
    "            df.write.format('parquet').partitionBy('BusDate','attendence_id').save(rawzonepath,mode='append')\n",
    "            partitionAlterSql = 'ALTER TABLE etl.' + sourceTableName + ' ADD IF NOT EXISTS PARTITION (' + 'BusDate' + \"=\" + str(Date) + \",\" + 'attendence_id' + \"=\" + str(attendence_id) +\") LOCATION '\" + rawzonepath + '/' + 'BusDate' + \"=\" + str(Date) + '/' + 'attendence_id' + \"=\" + str(attendence_id) +\"'\"\n",
    "            logging.info(partitionAlterSql)\n",
    "        elif(fileformat == 'json' or fileformat == 'avro' or fileformat == 'parquet' or fileformat == 'orc'):\n",
    "                    df.write.format(fileformat).partitionBy('BusDate','attendence_id').option('delimiter', filedelimiter).save(rawzonepath,mode='append')\n",
    "                    partitionAlterSql = 'ALTER TABLE etl.' + sourceTableName + ' ADD IF NOT EXISTS PARTITION (' + 'BusDate' + \"=\" + str(Date) + \",\" + 'attendence_id' + \"=\" + str(attendence_id) +\") LOCATION '\" + rawzonepath + '/' + 'BusDate' + \"=\" + str(Date) + '/' + 'attendence_id' + \"=\" + str(attendence_id) +\"'\"\n",
    "                    print(partitionAlterSql)\n",
    "        rawzonelocation = rawzonepath + '/' + 'BusDate' + \"=\" + str(Date) + '/' + 'attendence_id' + \"=\" + str(attendence_id)\n",
    "        logging.info('path to rawzone with partitions : %s',rawzonelocation)\n",
    "        return rawzonelocation\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error('Error occured while in function IngestionIntoRawzone', exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here in the fileformate check with htf as N\n",
      "2019-05-29\n"
     ]
    }
   ],
   "source": [
    "logging.info('fileformat check')\n",
    "if(fileformat == 'database'):\n",
    "    try:\n",
    "        logging.info('header_trailer_flag is N and fileformat is %s , hence calling MySqlDB',fileformat)\n",
    "        MySqlDB(attendence_id,feedname, filename, jsonpath,processType,busdate_calender) \n",
    "        logging.info('Job is SUCCESS')\n",
    "        from datetime import datetime\n",
    "        logging.info('Time taken is: %s',datetime.now() - startTime) \n",
    "    except Exception as e:\n",
    "        logging.error('Error occured while in function MySqlDB', exc_info=True) \n",
    "        logging.info('Job is Failed')\n",
    "        from datetime import datetime\n",
    "        logging.info('Time taken is: %s',datetime.now() - startTime)\n",
    "elif(fileformat == \"DELIMITED\"):\n",
    "    logging.info('Job is at delimiter check')\n",
    "    if(header_trailer_flag == 'Y'):\n",
    "        busdateposition = filemetadata.collect()[0]['busdateposition']\n",
    "        busdateposition= int(busdateposition)\n",
    "        numofrows = filemetadata.collect()[0]['trailercountpos']\n",
    "        numofrows = int(numofrows)\n",
    "        headerIdentifier = filemetadata.collect()[0]['headeridentifier']\n",
    "        detailIdentifier = filemetadata.collect()[0]['detailidentifier']\n",
    "        tailerIdentifier = filemetadata.collect()[0]['traileridentifier']\n",
    "        WithHeaderTrailer(attendence_id,feedname, filename, path, filedelimiter, ArrivalTimeStamp, FileSize, busdateposition, numofrows, busdate_calender,processType,fileformat,hId,dId,tId)\n",
    "    elif(header_trailer_flag == 'N'):\n",
    "        print('here in the fileformate check with htf as N')\n",
    "        WithOutHeaderTrailer(attendence_id,feedname, filename, path,filedelimiter, ArrivalTimeStamp, FileSize,busdate_calender,processType)   \n",
    "\n",
    "elif(fileformat == 'json' or fileformat == 'avro' or fileformat == 'parquet' or fileformat == 'orc'):\n",
    "    try:\n",
    "        logging.info('header_trailer_flag is N and fileformat is %s , hence calling WithOutHeaderTrailer',fileformat)\n",
    "        WithOutHeaderTrailer(attendence_id,feedname, filename, path, filetype, filedelimiter, ArrivalTimeStamp, FileSize, busdate_calender,processType,fileformat)\n",
    "    except Exception as e:\n",
    "        logging.error('Error occured while in function WithOutHeaderTrailer', exc_info=True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def funtion1(name):\n",
    "    return name+'+hi'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = funtion1('bhagya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bhagya+hi\n"
     ]
    }
   ],
   "source": [
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file://bhagya\n"
     ]
    }
   ],
   "source": [
    "path= 'bhagya'\n",
    "a = 'file://'+path\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
