{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import os, time  #import statements\n",
    "from datetime import datetime\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from stat import *\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import substring\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/local/bin/python3\"\n",
    "startTime = datetime.now()\n",
    "\n",
    "import datetime\n",
    "ts = time.time()\n",
    "currentTimeStamp = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "logging.basicConfig(filename='Ingestion_'+currentTimeStamp+'.log', filemode='w',format='%(asctime)s - %(message)s',datefmt='%d-%b-%y %H:%M:%S', level=logging.INFO)\n",
    "logging.info('Job Started : %s',currentTimeStamp)\n",
    "\n",
    "sparkSession = (SparkSession.builder.appName('pyspark-to-load-tables-hive').enableHiveSupport().getOrCreate())\n",
    "spark = SparkSession.builder.appName('changeColNames').getOrCreate()\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "logging.info('User logged in and created SparkContext and SparkSession')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedname = 'Bank'#sys.argv[1]''\n",
    "\n",
    "\n",
    "jsonpath = \"/home/etl/ETL/mysqlDetails.json\"\n",
    "data = pd.read_json(jsonpath,typ='series',orient='columns')\n",
    "DB_name= data.DB_name\n",
    "user=data.user\n",
    "password=data.password\n",
    "host=data.host\n",
    "\n",
    "\n",
    "conn = mysql.connector.connect(\n",
    "     host=host,\n",
    "     database=DB_name,\n",
    "     user=user,\n",
    "     password=password) \n",
    "\n",
    "feedControlquery= 'SELECT * FROM etl.feedcontrol WHERE feedname' + \" = '\" + feedname + \"'\"\n",
    "feedcontrol = pd.read_sql(feedControlquery, conn)\n",
    "feed = spark.createDataFrame(feedcontrol)\n",
    "\n",
    "\n",
    "calenderquery = 'SELECT * FROM etl.calender where openindicator =\"Y\" '\n",
    "calender_pd = pd.read_sql(calenderquery, conn)\n",
    "calender = spark.createDataFrame(calender_pd)\n",
    "\n",
    "filemetadataquery = 'SELECT * FROM etl.filemetadata' + ' WHERE feedname' + \" ='\" + feedname + \"'\"\n",
    "filemetadata_pd = pd.read_sql(filemetadataquery, conn)\n",
    "filemetadata = spark.createDataFrame(filemetadata_pd)\n",
    "\n",
    "UniqueConstraints = pd.read_sql('select * from mapper where feedname = \"UniqueConstraints\" ', conn)\n",
    "UniqueConstraints = spark.createDataFrame(UniqueConstraints)\n",
    "rawzonepath = UniqueConstraints.filter(UniqueConstraints.key_col == 'rawzonepath').collect()[0]['value_col']\n",
    "hiveThriftUrl = UniqueConstraints.filter(UniqueConstraints.key_col == 'hive-thrift-url').collect()[0]['value_col']\n",
    "\n",
    "\n",
    "\n",
    "SparkContext.setSystemProperty(\"hive.metastore.uris\", hiveThriftUrl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "24\n",
      "Bank\n",
      "CreditCard_Nulls.csv\n",
      "/home/etl/ETL/inputdata\n",
      "CreditCard_hivetb\n",
      "Y\n",
      "DELIMITED\n",
      "hdfs://10.3.2.13:8020/user/hadoop/rawzone\n",
      "ETL\n",
      "2019-05-29\n",
      "filedelimiter : ,\n",
      "/home/etl/ETL/inputdata/CreditCard_Nulls.csv\n",
      "rawpath:hdfs://10.3.2.13:8020/user/hadoop/rawzone/CreditCard_Nulls.csv \n"
     ]
    }
   ],
   "source": [
    "df_ATracker = sparkSession.sql('SELECT max(trackingid) as attendence_id FROM ETL.Attendence_Tracker order by attendence_id ')  #Attendence_tracker\n",
    "if(df_ATracker.collect()[0]['attendence_id']== None):\n",
    "    attendence_id = 1\n",
    "else:\n",
    "    attendence_id = int(df_ATracker.collect()[0]['attendence_id']) \n",
    "    attendence_id+=1 \n",
    "logging.info('value of attendence_id is %s', attendence_id)\n",
    "print(df_ATracker.collect()[0]['attendence_id'])\n",
    "print(attendence_id)\n",
    "feedname = feed.collect()[0]['feedname']\n",
    "print(feedname)\n",
    "filename = feed.collect()[0]['filename']\n",
    "print(filename)\n",
    "landingpath = feed.collect()[0]['landingpath']\n",
    "print(landingpath)\n",
    "sourceTableName = feed.collect()[0]['sourcetablename']\n",
    "print(sourceTableName)\n",
    "header_trailer_flag = feed.collect()[0]['headertrailerflag']\n",
    "print(header_trailer_flag)\n",
    "fileformat = feed.collect()[0]['dataformat']\n",
    "print(fileformat)\n",
    "print(rawzonepath)\n",
    "processType = feed.collect()[0]['processtype']\n",
    "print(processType)\n",
    "busdate_calender = calender.collect()[0]['busdate']\n",
    "print(busdate_calender)\n",
    "if(fileformat == 'DELIMITED' or fileformat == 'FIXED'):\n",
    "    filedelimiter = filemetadata.collect()[0]['filedelimiter']\n",
    "    print(\"filedelimiter : %s\" %filedelimiter)\n",
    "    path = str(\"%s/%s\" % (landingpath, filename))\n",
    "    print(path)\n",
    "else:\n",
    "    path = str(\"%s/%s%s%s\" % (landingpath, filename,\".\",fileformat))\n",
    "    print(path)\n",
    "if(fileformat == 'database'):\n",
    "    jsonpath = \"/home/etl/ETL/mysqlDetails.json\"\n",
    "    ArrivalTimeStamp=currentTimeStamp\n",
    "    FileSize = 0\n",
    "    logging.info('ArrivalTimestamp for MysqlDB fileformat %s',ArrivalTimeStamp)\n",
    "    rawzonepath = str(\"%s/%s\" % (rawzonepath, feedname))\n",
    "else:     #extracting arrival time stamp of file from system\n",
    "    ArrivalTimeStamp = time.ctime(os.path.getctime(path))\n",
    "    st = os.stat(path)\n",
    "    FileSize = st[ST_SIZE]\n",
    "    rawzonepath = str(\"%s/%s\" % (rawzonepath, filename))\n",
    "print(\"rawpath:%s \" %rawzonepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/home/etl/ETL/dataframecheck already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o318.save.\n: org.apache.spark.sql.AnalysisException: path file:/home/etl/ETL/dataframecheck already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fb97815d03c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mDetail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDetail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/etl/ETL/dataframecheck'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#['200229/05/2019abc56792', '200329/05/2019abc55685', '200429/05/2019abc54646']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path file:/home/etl/ETL/dataframecheck already exists.;'"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile('/home/etl/ETL/inputdata/CreditCard_Nulls.csv')             #EmployeeDetails.csv')     #CreditCard_Nulls.csv'\n",
    "H = lines.filter(lambda l: l.startswith('1')) \n",
    "H.collect()\n",
    "header = H.take(1)  #extracting header data\n",
    "Header = ''.join(header)    \n",
    "#Date = Header[5-1:14-1] #extracting business date\n",
    "\n",
    "Detail = lines.filter(lambda l: l.startswith('2'))  #separating detail data\n",
    "Detail.collect()\n",
    "Detail.take(10)\n",
    "dataframe = spark.createDataFrame(Detail, StringType())\n",
    "dataframe.write.format('csv').save('/home/etl/ETL/dataframecheck')\n",
    "#['200229/05/2019abc56792', '200329/05/2019abc55685', '200429/05/2019abc54646']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WithHeaderTrailer(attendence_id,feedname, filename, path, filedelimiter, ArrivalTimeStamp, FileSize, busdateposition, numofrowspos, busdate_calender,processType,fileformat,headerIdentifier,detailIdentifier,trailerIdentifier):\n",
    "        logging.info('Job is in function WithHeaderTrailer')\n",
    "        lines = sc.textFile(path)\n",
    "        logging.info(type(headerIdentifier))\n",
    "        H = lines.filter(lambda l: l.startswith(headerIdentifier)) \n",
    "        \n",
    "        H.collect()\n",
    "        header = H.take(1)  #extracting header data\n",
    "        Header = ''.join(header)    \n",
    "        Date = H.map(lambda l: l.split(filedelimiter)[busdateposition-1]) #extracting business date\n",
    "        DateOfExtract = Date.collect().pop(0)\n",
    "        Detail = lines.filter(lambda l: l.startswith(detailIdentifier))  #separating detail data\n",
    "        Detail.collect()\n",
    "        Detail_Count = Detail.count() #counting no of rows\n",
    "        print(Detail_Count)\n",
    "        T = lines.filter(lambda l: l.startswith(trailerIdentifier))  #separating trailer data\n",
    "        T.collect()\n",
    "        trailer = T.take(1)  #extracting trailer data\n",
    "        Trailer = ''.join(trailer)\n",
    "        NOR = T.map(lambda l: l.split(filedelimiter)[numofrowspos-1]) #extracting date of number of rows\n",
    "        NoOfRecords = int(NOR.collect().pop(0))\n",
    "        print(NoOfRecords) #validating date of extract\n",
    "        if DateOfExtract == busdate_calender:\n",
    "            HeaderVldFlag = 'Y'\n",
    "            ErrorCodeList = 'No Error'\n",
    "        else:\n",
    "            HeaderVldFlag = 'N'\n",
    "            ErrorCodeList = 'Invalid Busdate'\n",
    "        if NoOfRecords == Detail_Count: \n",
    "    #validating number of rows\n",
    "            TrailerVldFlag = 'Y'\n",
    "            ErrorCodeList = 'No Error'\n",
    "        else:\n",
    "            TrailerVldFlag = 'N'\n",
    "            ErrorCodeList = 'Invalid NoOfRecords'          \n",
    "        \n",
    "        \n",
    "        dataframe = spark.createDataFrame(Detail, StringType())\n",
    "        #data = dataframe.replace(\"\\\"\",\"\")                   #.map( lambda elem: elem.strip('\\\"'))\n",
    "        #data.show()\n",
    "        #data.write.format('csv').save('/home/etl/ETL/dataframecheck')\n",
    "        #dataframe = spark.createDataFrame(Detail, StringType())\n",
    "        dataframe.show()\n",
    "        #Ingestion Into Rawzone\n",
    "        rawzonelocation = IngestionIntoRawzone(attendence_id,processType,fileformat,DateOfExtract,dataframe)\n",
    "        #attendence tracker table\n",
    "        if(rawzonelocation != None):\n",
    "            Attendence_tracking(attendence_id, feedname, filename, path, ArrivalTimeStamp, currentTimeStamp,FileSize, DateOfExtract,rawzonelocation, Header, Trailer, HeaderVldFlag, TrailerVldFlag, ErrorCodeList)\n",
    "            logging.info('Data has been entered into Attendence_tracker')\n",
    "            \n",
    "            \n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixedWithHeaderTrailer(attendence_id,feedname, filename, path, layoutfilepath, ArrivalTimeStamp, FileSize, busdatestartpos, busdateendpos, totalrowsstartpos, totalrowsendpos, busdate_calender,processType,fileformat,headerIdentifier,detailIdentifier,trailerIdentifier):\n",
    "        logging.info('Job is in function WithHeaderTrailer')\n",
    "        lines = sc.textFile(path)\n",
    "        H = lines.filter(lambda l: l.startswith(headerIdentifier)) \n",
    "        H.collect()\n",
    "        header = H.take(1)  #extracting header data\n",
    "        Header = ''.join(header)    \n",
    "        Date = Header[busdatestartpos-1:busdateendpos-1] #extracting business date\n",
    "\n",
    "        Detail = lines.filter(lambda l: l.startswith(detailIdentifier))  #separating detail data\n",
    "        Detail.collect()\n",
    "        Detail_Count = Detail.count() #counting no of rows\n",
    "        print(Detail_Count)\n",
    "        Detail_dataframe = spark.createDataFrame(Detail, StringType())\n",
    "        Detail_dataframe.show()\n",
    "        \n",
    "        T = lines.filter(lambda l: l.startswith(trailerIdentifier))  #separating trailer data\n",
    "        T.collect()\n",
    "        trailer = T.take(1)  #extracting trailer data\n",
    "        Trailer = ''.join(trailer)\n",
    "        NOR = int(Trailer[totalrowsstartpos-1:totalrowsendtpos-1]) #extracting date of number of rows\n",
    "        print(NOR) #validating date of extract\n",
    "        \n",
    "        \n",
    "        if Date == busdate_calender:\n",
    "            HeaderVldFlag = 'Y'\n",
    "            ErrorCodeList = 'No Error'\n",
    "        else:\n",
    "            HeaderVldFlag = 'N'\n",
    "            ErrorCodeList = 'Invalid Busdate'\n",
    "        if NoOfRecords == Detail_Count: \n",
    "    #validating number of rows\n",
    "            TrailerVldFlag = 'Y'\n",
    "            ErrorCodeList = 'No Error'\n",
    "        else:\n",
    "            TrailerVldFlag = 'N'\n",
    "            ErrorCodeList = 'Invalid NoOfRecords'          \n",
    "        \n",
    "        SchemaFile = spark.read.format(\"json\").option(\"header\",\"true\").json(layoutfilepath)\n",
    "        sfDict = map(lambda x: x.asDict(), SchemaFile.collect())\n",
    "        type(sfDict)\n",
    "        dataframe = Detail_dataframe.select(\n",
    "            *[\n",
    "                substring(\n",
    "                    str='_c0',\n",
    "                    pos=int(row['From']),\n",
    "                    len=int(row['To'])\n",
    "                ).alias(row['Column']) \n",
    "                for row in sfDict\n",
    "            ]\n",
    "        )\n",
    "        rawzonelocation = IngestionIntoRawzone(attendence_id,processType,fileformat,busdate_calender,dataframe)\n",
    "        logging.info(rawzonelocation)\n",
    "        if(rawzonelocation != None):\n",
    "            Attendence_tracking(attendence_id,feedname, filename, path, ArrivalTimeStamp,currentTimeStamp, FileSize, busdate_calender,rawzonelocation,Header, Trailer, HeaderVldFlag, TrailerVldFlag, ErrorCodeList)\n",
    "            logging.info('Data has been entered into Attendence_tracker')\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(path)\n",
    "H = lines.filter(lambda l: l.startswith('1')) \n",
    "H.collect()\n",
    "header = H.take(1)  #extracting header data\n",
    "Header = ''.join(header)    \n",
    "Date = H.map(lambda l: l.split(filedelimiter)[busdateposition-1]) #extracting business date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WithOutHeaderTrailer(attendence_id,feedname, filename, path,filedelimiter, ArrivalTimeStamp, FileSize,busdate_calender,processType):\n",
    "        logging.info('Job is in function WithOutHeaderTrailer')\n",
    "        dataframe = spark.read.load(path, format='csv', sep=filedelimiter)\n",
    "        #Ingestion Into Rawzone\n",
    "        rawzonelocation = IngestionIntoRawzone(attendence_id,processType,fileformat,busdate_calender,dataframe)\n",
    "        #attendence tracker table\n",
    "        logging.info(rawzonelocation)\n",
    "        if(rawzonelocation != None):\n",
    "            Attendence_tracking(attendence_id,feedname, filename, path, ArrivalTimeStamp,currentTimeStamp, FileSize, busdate_calender,rawzonelocation,\"null\",\"null\",\"null\",\"null\",\"null\")\n",
    "            logging.info('Data has been entered into Attendence_tracker')\n",
    "            \n",
    "            \n",
    "         \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixedWithOutHeaderTrailer(attendence_id,feedname,filename,path,layoutfilepath,ArrivalTimeStamp,FileSize,busdate_calender,processType):\n",
    "    logging.info('Job is in function WithOutHeaderTrailer')\n",
    "    #layoutFile='/home/etl/ETL/inputdata/fixed_length.json' ##should come from mapping tbl\n",
    "    #inputFile = '/home/etl/ETL/inputdata/fixed_length.dat'  ##should come from feedcontrol\n",
    "    \n",
    "    SchemaFile = spark.read\\\n",
    "        .format(\"json\")\\\n",
    "        .option(\"header\",\"true\")\\\n",
    "        .json(layoutfilepath)\n",
    "    \n",
    "    File = spark.read\\\n",
    "        .format(\"csv\")\\\n",
    "        .option(\"header\",\"false\")\\\n",
    "        .load(path)\n",
    "    \n",
    "    File.show()\n",
    "    \n",
    "    sfDict = map(lambda x: x.asDict(), SchemaFile.collect())\n",
    "    \n",
    "    type(sfDict)\n",
    "    \n",
    "    dataframe = File.select(\n",
    "            *[\n",
    "                substring(\n",
    "                    str='_c0',\n",
    "                    pos=int(row['From']),\n",
    "                    len=int(row['To'])\n",
    "                ).alias(row['Column']) \n",
    "                for row in sfDict\n",
    "            ]\n",
    "        )\n",
    "    rawzonelocation = IngestionIntoRawzone(attendence_id,processType,fileformat,busdate_calender,dataframe)\n",
    "    logging.info(rawzonelocation)\n",
    "    if(rawzonelocation != None):\n",
    "        Attendence_tracking(attendence_id,feedname, filename, path, ArrivalTimeStamp,currentTimeStamp, FileSize, busdate_calender,rawzonelocation,\"null\",\"null\",\"null\",\"null\",\"null\")\n",
    "        logging.info('Data has been entered into Attendence_tracker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MySqlDB(attendence_id,feedname, filename, jsonpath,processType,busdate_calender):\n",
    "    logging.info('Job is in function MySqlDB')\n",
    "    data = pd.read_json(jsonpath,typ='series',orient='columns')\n",
    "    DB_name= data.DB_name\n",
    "    user=data.user\n",
    "    password=data.password\n",
    "    host=data.host\n",
    "    query=data.query\n",
    "    conn = mysql.connector.connect(\n",
    "         host=host,\n",
    "         database=DB_name,\n",
    "         user=user,\n",
    "         password=password)\n",
    "    #attendence tracker table \n",
    "    pd_df = pd.read_sql(query, conn)\n",
    "    dataframe = spark.createDataFrame(pd_df)\n",
    "    dataframe.show()\n",
    "    rawzonelocation = IngestionIntoRawzone(attendence_id,processType,fileformat,busdate_calender,dataframe)\n",
    "    print(rawzonelocation)\n",
    "    if(rawzonelocation != None):\n",
    "        Attendence_tracking(attendence_id,feedname, filename, \"\", ArrivalTimeStamp,currentTimeStamp, FileSize, busdate_calender,rawzonelocation,\"\",\"\",\"\",\"\",\"\")    \n",
    "        logging.info('Data has been entered into Attendence_tracker')\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attendence_tracking(attendence_id, feedname, filename, path, ArrivalTimeStamp, currentTimeStamp,FileSize, DateOfExtract,rawzonepath, Header, Trailer, HeaderVldFlag, TrailerVldFlag, ErrorCodeList):\n",
    "    sparkSession.sql(\"insert into table etl.attendence_tracker select '{}','{}','{}','{}','{}','{}','{}','{}','{}','{}','{}','{}','{}','{}'\".format(attendence_id, feedname, filename, path, ArrivalTimeStamp, currentTimeStamp,FileSize, DateOfExtract,rawzonepath, Header, Trailer, HeaderVldFlag, TrailerVldFlag, ErrorCodeList))\n",
    "\n",
    "def IngestionIntoRawzone(attendence_id,processType,fileformat,busdate_calender,dataframe):\n",
    "    try:\n",
    "        dataframe = dataframe.withColumn('attendence_id',f.lit(attendence_id))\n",
    "        #Date_date=datetime.datetime.strptime(busdate_calender, '%d/%m/%Y')\n",
    "        #print(busdate_calender)\n",
    "        Date=busdate_calender\n",
    "        print(Date)\n",
    "        df = dataframe.withColumn('BusDate',f.lit(Date))\n",
    "        if(fileformat == 'DELIMITED'):\n",
    "            df.write.format('csv').partitionBy('BusDate','attendence_id').option('delimiter', filedelimiter).save(rawzonepath, mode='append')\n",
    "            partitionAlterSql = 'ALTER TABLE etl.' + sourceTableName + ' ADD IF NOT EXISTS PARTITION (' + 'BusDate' + \"='\" + str(Date) + \"',\" + 'attendence_id' + \"=\" + str(attendence_id) +\") LOCATION '\" + rawzonepath + '/' + 'BusDate' + \"=\" + str(Date) + '/' + 'attendence_id' + \"=\" + str(attendence_id) +\"'\"\n",
    "            logging.info(partitionAlterSql)\n",
    "            sparkSession.sql(partitionAlterSql)\n",
    "        elif(fileformat == 'FIXED' ): \n",
    "            df.write.format('csv').partitionBy('BusDate','attendence_id').option('delimiter', ',').save(rawzonepath, mode='append')\n",
    "            partitionAlterSql = 'ALTER TABLE etl.' + sourceTableName + ' ADD IF NOT EXISTS PARTITION (' + 'BusDate' + \"='\" + str(Date) + \"',\" + 'attendence_id' + \"=\" + str(attendence_id) +\") LOCATION '\" + rawzonepath + '/' + 'BusDate' + \"=\" + str(Date) + '/' + 'attendence_id' + \"=\" + str(attendence_id) +\"'\"\n",
    "            logging.info(partitionAlterSql)\n",
    "            sparkSession.sql(partitionAlterSql)\n",
    "        elif(fileformat == 'database'):\n",
    "            df.write.format('parquet').partitionBy('BusDate','attendence_id').save(rawzonepath,mode='append')\n",
    "            partitionAlterSql = 'ALTER TABLE etl.' + sourceTableName + ' ADD IF NOT EXISTS PARTITION (' + 'BusDate' + \"=\" + str(Date) + \",\" + 'attendence_id' + \"=\" + str(attendence_id) +\") LOCATION '\" + rawzonepath + '/' + 'BusDate' + \"=\" + str(Date) + '/' + 'attendence_id' + \"=\" + str(attendence_id) +\"'\"\n",
    "            logging.info(partitionAlterSql)\n",
    "        elif(fileformat == 'json' or fileformat == 'avro' or fileformat == 'parquet' or fileformat == 'orc'):\n",
    "                    df.write.format(fileformat).partitionBy('BusDate','attendence_id').option('delimiter', filedelimiter).save(rawzonepath,mode='append')\n",
    "                    partitionAlterSql = 'ALTER TABLE etl.' + sourceTableName + ' ADD IF NOT EXISTS PARTITION (' + 'BusDate' + \"=\" + str(Date) + \",\" + 'attendence_id' + \"=\" + str(attendence_id) +\") LOCATION '\" + rawzonepath + '/' + 'BusDate' + \"=\" + str(Date) + '/' + 'attendence_id' + \"=\" + str(attendence_id) +\"'\"\n",
    "                    print(partitionAlterSql)\n",
    "        rawzonelocation = rawzonepath + '/' + 'BusDate' + \"=\" + str(Date) + '/' + 'attendence_id' + \"=\" + str(attendence_id)\n",
    "        logging.info('path to rawzone with partitions : %s',rawzonelocation)\n",
    "        return rawzonelocation\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error('Error occured while in function IngestionIntoRawzone', exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494\n",
      "494\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|2,7106-4239-7093-...|\n",
      "|2,6492-5655-8241-...|\n",
      "|2,2868-5606-5152-...|\n",
      "|2,1438-6906-2509-...|\n",
      "|2,2764-7023-8396-...|\n",
      "|2,4864-7119-5608-...|\n",
      "|2,5160-8427-6529-...|\n",
      "|2,6691-5105-1556-...|\n",
      "|2,1481-2536-2178-...|\n",
      "|2,1355-1728-8274-...|\n",
      "|2,9621-6787-7890-...|\n",
      "|2,6385-4594-8055-...|\n",
      "|2,2595-8621-2855-...|\n",
      "|2,7214-4915-6387-...|\n",
      "|2,7908-3850-6633-...|\n",
      "|2,6574-5513-6101-...|\n",
      "|2,4203-2936-4855-...|\n",
      "|2,6239-8641-8524-...|\n",
      "|2,1259-5241-3561-...|\n",
      "|2,1113-9175-3253-...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "29/05/2019\n",
      "hdfs://10.3.2.13:8020/user/hadoop/rawzone/CreditCard_Nulls.csv/BusDate=29/05/2019/attendence_id=24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "logging.info('fileformat check')\n",
    "if(fileformat == 'database'):\n",
    "    try:\n",
    "        logging.info('header_trailer_flag is N and fileformat is %s , hence calling MySqlDB',fileformat)\n",
    "        MySqlDB(attendence_id,feedname, filename, jsonpath,processType,busdate_calender) \n",
    "        logging.info('Job is SUCCESS')\n",
    "        from datetime import datetime\n",
    "        logging.info('Time taken is: %s',datetime.now() - startTime) \n",
    "    except Exception as e:\n",
    "        logging.error('Error occured while in function MySqlDB', exc_info=True) \n",
    "        logging.info('Job is Failed')\n",
    "        from datetime import datetime\n",
    "        logging.info('Time taken is: %s',datetime.now() - startTime)\n",
    "elif(fileformat == \"DELIMITED\"):\n",
    "    logging.info('Job is at delimiter check')\n",
    "    if(header_trailer_flag == 'Y'):\n",
    "        busdateposition = filemetadata.collect()[0]['busdateposition']\n",
    "        busdateposition= int(busdateposition)\n",
    "        numofrows = filemetadata.collect()[0]['trailercountpos']\n",
    "        numofrows = int(numofrows)\n",
    "        headerIdentifier = str(filemetadata.collect()[0]['headeridentifier'])\n",
    "        detailIdentifier = str(filemetadata.collect()[0]['detailidentifier'])\n",
    "        trailerIdentifier = str(filemetadata.collect()[0]['traileridentifier'])\n",
    "        try:\n",
    "            logging.info('header_trailer_flag is Y and fileformat is %s , hence calling WithHeaderTrailer',fileformat)\n",
    "            WithHeaderTrailer(attendence_id,feedname, filename, path, filedelimiter, ArrivalTimeStamp, FileSize, busdateposition, numofrows, busdate_calender,processType,fileformat,headerIdentifier,detailIdentifier,trailerIdentifier)\n",
    "        except Exception as e:\n",
    "            logging.error('Error occured while in function WithHeaderTrailer with header_trailer_flag is Y and fileformat is DELIMITED', exc_info=True)\n",
    "    elif(header_trailer_flag == 'N'):\n",
    "        try:\n",
    "            logging.info('header_trailer_flag is N and fileformat is %s , hence calling WithOutHeaderTrailer',fileformat)\n",
    "            WithOutHeaderTrailer(attendence_id,feedname, filename, path,filedelimiter, ArrivalTimeStamp, FileSize,busdate_calender,processType)\n",
    "        except Exception as e:\n",
    "            logging.error('Error occured while in function WithOutHeaderTrailer', exc_info=True)    \n",
    "\n",
    "elif(fileformat == 'json' or fileformat == 'avro' or fileformat == 'parquet' or fileformat == 'orc'):\n",
    "    try:\n",
    "        logging.info('header_trailer_flag is N and fileformat is %s , hence calling WithOutHeaderTrailer',fileformat)\n",
    "        WithOutHeaderTrailer(attendence_id,feedname, filename, path, filetype, filedelimiter, ArrivalTimeStamp, FileSize, busdate_calender,processType,fileformat)\n",
    "    except Exception as e:\n",
    "        logging.error('Error occured while in function WithOutHeaderTrailer', exc_info=True)\n",
    "        \n",
    "elif(fileformat == \"FIXED\"):\n",
    "    mapperquery = 'SELECT * FROM etl.mapper' + ' WHERE feedname' + \" ='\" + feedname + \"' and key_col = 'layoutfilepath'\"\n",
    "    print(mapperquery)\n",
    "    mapper_pd = pd.read_sql(mapperquery, conn)\n",
    "    mapper = spark.createDataFrame(mapper_pd)\n",
    "    layoutfilepath = mapper.collect()[0]['value_col']\n",
    "    print(layoutfilepath)\n",
    "    if(header_trailer_flag == 'Y'):\n",
    "        busdatestartpos = filemetadata.collect()[0]['busdatestartpos']\n",
    "        busdateendpos = filemetadata.collect()[0]['busdateendpos']\n",
    "        totalrowsstartpos = filemetadata.collect()[0]['totalrowsstartpos']\n",
    "        totalrowsendpos = filemetadata.collect()[0]['totalrowsendpos']\n",
    "        numofrows = int(numofrows)\n",
    "        headerIdentifier = str(filemetadata.collect()[0]['headeridentifier'])\n",
    "        detailIdentifier = str(filemetadata.collect()[0]['detailidentifier'])\n",
    "        trailerIdentifier = str(filemetadata.collect()[0]['traileridentifier'])\n",
    "        try:\n",
    "            logging.info('header_trailer_flag is Y and fileformat is %s , hence calling WithHeaderTrailer',fileformat)\n",
    "            FixedWithHeaderTrailer(attendence_id,feedname, filename, path, layoutfilepath, ArrivalTimeStamp, FileSize, busdatestartpos, busdateendpos, totalrowsstartpos, totalrowsendpos, busdate_calender,processType,fileformat,headerIdentifier,detailIdentifier,trailerIdentifier)\n",
    "        except Exception as e:\n",
    "            logging.error('Error occured while in function WithHeaderTrailer with header_trailer_flag is Y and fileformat is DELIMITED', exc_info=True)\n",
    "    elif(header_trailer_flag == 'N'):\n",
    "        try:\n",
    "            logging.info('header_trailer_flag is N and fileformat is %s , hence calling WithOutHeaderTrailer',fileformat)\n",
    "            FixedWithOutHeaderTrailer(attendence_id,feedname, filename, path,layoutfilepath, ArrivalTimeStamp, FileSize,busdate_calender,processType)\n",
    "        except Exception as e:\n",
    "            logging.error('Error occured while in function FixedWithOutHeaderTrailer', exc_info=True)    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def funtion1(name):\n",
    "    return name+'+hi'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = funtion1('bhagya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bhagya+hi\n"
     ]
    }
   ],
   "source": [
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file://bhagya\n"
     ]
    }
   ],
   "source": [
    "path= 'bhagya'\n",
    "a = 'file://'+path\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
