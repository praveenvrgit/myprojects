{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import os, time  #import statements\n",
    "from datetime import datetime\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from stat import *\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/local/bin/python3\"\n",
    "startTime = datetime.now()\n",
    "\n",
    "import datetime\n",
    "ts = time.time()\n",
    "currentTimeStamp = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "logging.basicConfig(filename='Ingestion_'+currentTimeStamp+'.log', filemode='w',format='%(asctime)s - %(message)s',datefmt='%d-%b-%y %H:%M:%S', level=logging.INFO)\n",
    "logging.info('Job Started : %s',currentTimeStamp)\n",
    "SparkContext.setSystemProperty(\"hive.metastore.uris\", \"thrift://10.3.2.20:9083\")\n",
    "sparkSession = (SparkSession.builder.appName('pyspark-to-load-tables-hive').enableHiveSupport().getOrCreate())\n",
    "spark = SparkSession.builder.appName('changeColNames').getOrCreate()\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "logging.info('User logged in and created SparkContext and SparkSession')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedname = 'Bank'#sys.argv[1]''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+--------------------+---------------+-----------+-----------------+--------------------+-----------------+\n",
      "|feedname|            filename|fileformat|         landingpath|rejectthreshold|processtype|headertrailerflag|         rawzonepath|  sourcetablename|\n",
      "+--------+--------------------+----------+--------------------+---------------+-----------+-----------------+--------------------+-----------------+\n",
      "|    Bank|CreditCard_Nulls.csv| DELIMITED|/home/etl/ETL/inp...|              3|        ETL|                Y|hdfs://10.3.2.13:...|CreditCard_hivetb|\n",
      "+--------+--------------------+----------+--------------------+---------------+-----------+-----------------+--------------------+-----------------+\n",
      "\n",
      "+----------+-------------+\n",
      "|   busdate|openindicator|\n",
      "+----------+-------------+\n",
      "|29/05/2019|            Y|\n",
      "+----------+-------------+\n",
      "\n",
      "+--------+----------------+---------+-------------+---------------+---------------+-------------+-------------+---------------+----------------+----------------+-----------------+\n",
      "|feedname|        filename| filetype|filedelimiter|busdateposition|busdatestartpos|busdateendpos|busdateformat|trailercountpos|headeridentifier|detailidentifier|traileridentifier|\n",
      "+--------+----------------+---------+-------------+---------------+---------------+-------------+-------------+---------------+----------------+----------------+-----------------+\n",
      "|    Bank|CreditCard_Nulls|DELIMITED|            ,|              3|           null|         null|   DD/MM/YYYY|              2|               1|               2|                3|\n",
      "+--------+----------------+---------+-------------+---------------+---------------+-------------+-------------+---------------+----------------+----------------+-----------------+\n",
      "\n",
      "2\n",
      "3\n",
      "Bank\n",
      "CreditCard_Nulls.csv\n",
      "/home/etl/ETL/inputdata\n",
      "CreditCard_hivetb\n",
      "Y\n",
      "DELIMITED\n",
      "hdfs://10.3.2.13:8020/user/hadoop/rawzone/\n",
      "ETL\n",
      "29/05/2019\n",
      "/home/etl/ETL/inputdata/CreditCard_Nulls.csv\n",
      "rawpath:hdfs://10.3.2.13:8020/user/hadoop/rawzone/CreditCard_Nulls.csv \n"
     ]
    }
   ],
   "source": [
    "logging.info('Feteched command line arguments %s',feedname)\n",
    "try:\n",
    "    selectSql = 'SELECT * FROM etl.feedcontrol' + ' WHERE feedname' + \" ='\" + feedname + \"'\"\n",
    "    logging.info('%s',selectSql)\n",
    "    feed = sparkSession.sql(selectSql)\n",
    "    feed.show()\n",
    "    cal = sparkSession.sql('SELECT * FROM etl.calender where openindicator =\"Y\" ')  #calender table\n",
    "    cal.show()\n",
    "    filemetadata_selectSql = 'SELECT * FROM etl.filemetadata' + ' WHERE feedname' + \" ='\" + feedname + \"'\"\n",
    "    filemetadata = sparkSession.sql(filemetadata_selectSql)\n",
    "    filemetadata.show()\n",
    "    try:\n",
    "        df_ATracker = sparkSession.sql('SELECT max(trackingid) as attendence_id FROM ETL.Attendence_Tracker order by attendence_id ')  #Attendence_tracker\n",
    "        if(df_ATracker.collect()[0]['attendence_id']== None):\n",
    "           attendence_id = 1\n",
    "        else:\n",
    "           attendence_id = int(df_ATracker.collect()[0]['attendence_id']) \n",
    "           attendence_id+=1 \n",
    "        logging.info('value of attendence_id is %s', attendence_id)\n",
    "        print(df_ATracker.collect()[0]['attendence_id'])\n",
    "        print(attendence_id)\n",
    "    except Exception as e:\n",
    "        logging.error('Error occured while creating dataframe for attendence_tarcker tables', exc_info=True)\n",
    "    feedname = feed.collect()[0]['feedname']\n",
    "    print(feedname)\n",
    "    filename = feed.collect()[0]['filename']\n",
    "    print(filename)\n",
    "    landingpath = feed.collect()[0]['landingpath']\n",
    "    print(landingpath)\n",
    "    sourceTableName = feed.collect()[0]['sourcetablename']\n",
    "    print(sourceTableName)\n",
    "    header_trailer_flag = feed.collect()[0]['headertrailerflag']\n",
    "    print(header_trailer_flag)\n",
    "    fileformat = feed.collect()[0]['fileformat']\n",
    "    print(fileformat)\n",
    "    rawzonepath = feed.collect()[0]['rawzonepath']\n",
    "    print(rawzonepath)\n",
    "    processType = feed.collect()[0]['processtype']\n",
    "    print(processType)\n",
    "    busdate_calender = cal.collect()[0]['busdate']\n",
    "    print(busdate_calender)\n",
    "    if(fileformat == 'DELIMITED' or fileformat == 'FIXED'):\n",
    "        filedelimiter = filemetadata.collect()[0]['filedelimiter']\n",
    "        path = str(\"%s/%s\" % (landingpath, filename))\n",
    "        print(path)\n",
    "    else:\n",
    "        path = str(\"%s/%s%s%s\" % (landingpath, filename,\".\",fileformat))\n",
    "        print(path)\n",
    "    if(fileformat == 'database'):\n",
    "        jsonpath = \"/home/etl/ETL/mysqlDetails.json\"\n",
    "        ArrivalTimeStamp=currentTimeStamp\n",
    "        FileSize = 0\n",
    "        logging.info('ArrivalTimestamp for MysqlDB fileformat %s',ArrivalTimeStamp)\n",
    "    else:     #extracting arrival time stamp of file from system\n",
    "        ArrivalTimeStamp = time.ctime(os.path.getctime(path))\n",
    "        st = os.stat(path)\n",
    "        FileSize = st[ST_SIZE]\n",
    "    rawzonepath = str(\"%s%s\" % (rawzonepath, filename))\n",
    "    print(\"rawpath:%s \" %rawzonepath)\n",
    "except Exception as e:\n",
    "    logging.info('Job is FAILED')\n",
    "    logging.error('Error occured while creating dataframe or extracting data from feedcontrol and calender tables', exc_info=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,CreditCard_Nulls,29/05/2019']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile('file:///home/etl/ETL/inputdata/CreditCard_Nulls.csv')\n",
    "H = lines.filter(lambda l: l.startswith('1')) \n",
    "H.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WithHeaderTrailer(attendence_id,feedname, filename, path, filedelimiter, ArrivalTimeStamp, FileSize, busdateposition, numofrowspos, busdate_calender,processType,fileformat):\n",
    "        logging.info('Job is in function WithHeaderTrailer')\n",
    "        lines = sc.textFile(path)\n",
    "        H = lines.filter(lambda l: l.startswith('1')) \n",
    "        H.collect()\n",
    "        header = H.take(1)  #extracting header data\n",
    "        Header = ''.join(header)    \n",
    "        Date = H.map(lambda l: l.split(filedelimiter)[busdateposition-1]) #extracting business date\n",
    "        DateOfExtract = Date.collect().pop(0)\n",
    "        Detail = lines.filter(lambda l: l.startswith('2'))  #separating detail data\n",
    "        Detail.collect()\n",
    "        \n",
    "        Detail_Count = Detail.count() #counting no of rows\n",
    "        print(Detail_Count)\n",
    "        T = lines.filter(lambda l: l.startswith('3'))  #separating trailer data\n",
    "        T.collect()\n",
    "        trailer = T.take(1)  #extracting trailer data\n",
    "        Trailer = ''.join(trailer)\n",
    "        NOR = T.map(lambda l: l.split(filedelimiter)[numofrowspos-1]) #extracting date of number of rows\n",
    "        NoOfRecords = int(NOR.collect().pop(0))\n",
    "        print(NoOfRecords) #validating date of extract\n",
    "        if DateOfExtract == busdate_calender:\n",
    "            HeaderVldFlag = 'Y'\n",
    "            ErrorCodeList = 'No Error'\n",
    "        else:\n",
    "            HeaderVldFlag = 'N'\n",
    "            ErrorCodeList = 'Invalid Busdate'\n",
    "        if NoOfRecords == Detail_Count: \n",
    "    #validating number of rows\n",
    "            TrailerVldFlag = 'Y'\n",
    "            ErrorCodeList = 'No Error'\n",
    "        else:\n",
    "            TrailerVldFlag = 'N'\n",
    "            ErrorCodeList = 'Invalid NoOfRecords'          \n",
    "        \n",
    "        \n",
    "        dataframe = spark.createDataFrame(Detail, StringType())\n",
    "        dataframe.show()\n",
    "        #Ingestion Into Rawzone\n",
    "        rawzonelocation = IngestionIntoRawzone(attendence_id,processType,fileformat,DateOfExtract,dataframe)\n",
    "        #attendence tracker table\n",
    "        print(rawzonelocation)\n",
    "        Attendence_tracking(attendence_id, feedname, filename, path, ArrivalTimeStamp, currentTimeStamp,FileSize, DateOfExtract,rawzonelocation, Header, Trailer, HeaderVldFlag, TrailerVldFlag, ErrorCodeList)\n",
    "        logging.info('Data has been entered into Attendence_tracker')\n",
    "            \n",
    "            \n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WithOutHeaderTrailer(attendence_id,feedname, filename, path,filedelimiter, ArrivalTimeStamp, FileSize,busdate_calender,processType):\n",
    "        logging.info('Job is in function WithOutHeaderTrailer')\n",
    "        dataframe = spark.read.load(path, format='csv', sep=filedelimiter)\n",
    "        #Ingestion Into Rawzone\n",
    "        rawzonelocation = IngestionIntoRawzone(attendence_id,processType,fileformat,busdate_calender,dataframe)\n",
    "        #attendence tracker table\n",
    "        logging.info(rawzonelocation)\n",
    "        Attendence_tracking(attendence_id,feedname, filename, path, ArrivalTimeStamp,currentTimeStamp, FileSize, busdate_calender,rawzonelocation,\"null\",\"null\",\"null\",\"null\",\"null\")\n",
    "        logging.info('Data has been entered into Attendence_tracker')\n",
    "            \n",
    "            \n",
    "         \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MySqlDB(attendence_id,feedname, filename, jsonpath,processType,busdate_calender):\n",
    "    logging.info('Job is in function MySqlDB')\n",
    "    data = pd.read_json(jsonpath,typ='series',orient='columns')\n",
    "    DB_name= data.DB_name\n",
    "    user=data.user\n",
    "    password=data.password\n",
    "    host=data.host\n",
    "    query=data.query\n",
    "    conn = mysql.connector.connect(\n",
    "         host=host,\n",
    "         database=DB_name,\n",
    "         user=user,\n",
    "         password=password)\n",
    "    #attendence tracker table \n",
    "    pd_df = pd.read_sql(query, conn)\n",
    "    dataframe = spark.createDataFrame(pd_df)\n",
    "    dataframe.show()\n",
    "    rawzonelocation = IngestionIntoRawzone(attendence_id,processType,fileformat,busdate_calender,dataframe)\n",
    "    print(rawzonelocation)\n",
    "    Attendence_tracking(attendence_id,feedname, filename, \"\", ArrivalTimeStamp,currentTimeStamp, FileSize, busdate_calender,rawzonelocation,\"\",\"\",\"\",\"\",\"\")    \n",
    "        \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attendence_tracking(attendence_id, feedname, filename, path, ArrivalTimeStamp, currentTimeStamp,FileSize, DateOfExtract,rawzonepath, Header, Trailer, HeaderVldFlag, TrailerVldFlag, ErrorCodeList):\n",
    "    sparkSession.sql(\"insert into table etl.attendence_tracker select '{}','{}','{}','{}','{}','{}','{}','{}','{}','{}','{}','{}','{}','{}'\".format(attendence_id, feedname, filename, path, ArrivalTimeStamp, currentTimeStamp,FileSize, DateOfExtract,rawzonepath, Header, Trailer, HeaderVldFlag, TrailerVldFlag, ErrorCodeList))\n",
    "\n",
    "def IngestionIntoRawzone(attendence_id,processType,fileformat,busdate_calender,dataframe):\n",
    "    try:\n",
    "        dataframe = dataframe.withColumn('attendence_id',f.lit(attendence_id))\n",
    "        Date_date=datetime.datetime.strptime(busdate_calender, '%d/%m/%Y')\n",
    "        print(busdate_calender)\n",
    "        Date=Date_date.date()\n",
    "        print(Date)\n",
    "        df = dataframe.withColumn('BusDate',f.lit(Date))\n",
    "        if(fileformat == 'DELIMITED'):\n",
    "            df.write.format('csv').partitionBy('BusDate','attendence_id').option('delimiter', filedelimiter).save(rawzonepath, mode='append')\n",
    "            partitionAlterSql = 'ALTER TABLE etl.' + sourceTableName + ' ADD IF NOT EXISTS PARTITION (' + 'BusDate' + \"='\" + str(Date) + \"',\" + 'attendence_id' + \"=\" + str(attendence_id) +\") LOCATION '\" + rawzonepath + '/' + 'BusDate' + \"=\" + str(Date) + '/' + 'attendence_id' + \"=\" + str(attendence_id) +\"'\"\n",
    "            logging.info(partitionAlterSql)\n",
    "            sparkSession.sql(partitionAlterSql)\n",
    "        elif(fileformat == 'database'):\n",
    "            df.write.format('parquet').partitionBy('BusDate','attendence_id').save(rawzonepath,mode='append')\n",
    "            partitionAlterSql = 'ALTER TABLE etl.' + sourceTableName + ' ADD IF NOT EXISTS PARTITION (' + 'BusDate' + \"=\" + str(Date) + \",\" + 'attendence_id' + \"=\" + str(attendence_id) +\") LOCATION '\" + rawzonepath + '/' + 'BusDate' + \"=\" + str(Date) + '/' + 'attendence_id' + \"=\" + str(attendence_id) +\"'\"\n",
    "            logging.info(partitionAlterSql)\n",
    "        elif(fileformat == 'json' or fileformat == 'avro' or fileformat == 'parquet' or fileformat == 'orc'):\n",
    "                    df.write.format(fileformat).partitionBy('BusDate','attendence_id').option('delimiter', filedelimiter).save(rawzonepath,mode='append')\n",
    "                    partitionAlterSql = 'ALTER TABLE etl.' + sourceTableName + ' ADD IF NOT EXISTS PARTITION (' + 'BusDate' + \"=\" + str(Date) + \",\" + 'attendence_id' + \"=\" + str(attendence_id) +\") LOCATION '\" + rawzonepath + '/' + 'BusDate' + \"=\" + str(Date) + '/' + 'attendence_id' + \"=\" + str(attendence_id) +\"'\"\n",
    "                    print(partitionAlterSql)\n",
    "        rawzonelocation = rawzonepath + '/' + 'BusDate' + \"=\" + str(Date) + '/' + 'attendence_id' + \"=\" + str(attendence_id)\n",
    "        logging.info('path to rawzone with partitions : %s',rawzonelocation)\n",
    "        return rawzonelocation\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error('Error occured while in function IngestionIntoRawzone', exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494\n",
      "494\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|2,7106-4239-7093-...|\n",
      "|2,6492-5655-8241-...|\n",
      "|2,2868-5606-5152-...|\n",
      "|2,1438-6906-2509-...|\n",
      "|2,2764-7023-8396-...|\n",
      "|2,4864-7119-5608-...|\n",
      "|2,5160-8427-6529-...|\n",
      "|2,6691-5105-1556-...|\n",
      "|2,1481-2536-2178-...|\n",
      "|2,1355-1728-8274-...|\n",
      "|2,9621-6787-7890-...|\n",
      "|2,6385-4594-8055-...|\n",
      "|2,2595-8621-2855-...|\n",
      "|2,7214-4915-6387-...|\n",
      "|2,7908-3850-6633-...|\n",
      "|2,6574-5513-6101-...|\n",
      "|2,4203-2936-4855-...|\n",
      "|2,6239-8641-8524-...|\n",
      "|2,1259-5241-3561-...|\n",
      "|2,1113-9175-3253-...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "29/05/2019\n",
      "2019-05-29\n",
      "hdfs://10.3.2.13:8020/user/hadoop/rawzone/CreditCard_Nulls.csv/BusDate=2019-05-29/attendence_id=3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "logging.info('fileformat check')\n",
    "if(fileformat == 'database'):\n",
    "    try:\n",
    "        logging.info('header_trailer_flag is N and fileformat is %s , hence calling MySqlDB',fileformat)\n",
    "        MySqlDB(attendence_id,feedname, filename, jsonpath,processType,busdate_calender) \n",
    "        logging.info('Job is SUCCESS')\n",
    "        from datetime import datetime\n",
    "        logging.info('Time taken is: %s',datetime.now() - startTime) \n",
    "    except Exception as e:\n",
    "        logging.error('Error occured while in function MySqlDB', exc_info=True) \n",
    "        logging.info('Job is Failed')\n",
    "        from datetime import datetime\n",
    "        logging.info('Time taken is: %s',datetime.now() - startTime)\n",
    "elif(fileformat == \"DELIMITED\"):\n",
    "    logging.info('Job is at delimiter check')\n",
    "    if(header_trailer_flag == 'Y'):\n",
    "        busdateposition = filemetadata.collect()[0]['busdateposition']\n",
    "        busdateposition= int(busdateposition)\n",
    "        numofrows = filemetadata.collect()[0]['trailercountpos']\n",
    "        numofrows = int(numofrows)\n",
    "        hId = filemetadata.collect()[0]['headeridentifier']\n",
    "        dId = filemetadata.collect()[0]['detailidentifier']\n",
    "        tId = filemetadata.collect()[0]['traileridentifier']\n",
    "        try:\n",
    "            logging.info('header_trailer_flag is Y and fileformat is %s , hence calling WithHeaderTrailer',fileformat)\n",
    "            WithHeaderTrailer(attendence_id,feedname, filename, path, filedelimiter, ArrivalTimeStamp, FileSize, busdateposition, numofrows, busdate_calender,processType,fileformat,hId,dId,tId)\n",
    "        except Exception as e:\n",
    "            logging.error('Error occured while in function WithHeaderTrailer with header_trailer_flag is Y and fileformat is DELIMITED', exc_info=True)\n",
    "    elif(header_trailer_flag == 'N'):\n",
    "        try:\n",
    "            logging.info('header_trailer_flag is N and fileformat is %s , hence calling WithOutHeaderTrailer',fileformat)\n",
    "            WithOutHeaderTrailer(attendence_id,feedname, filename, path,filedelimiter, ArrivalTimeStamp, FileSize,busdate_calender,processType)\n",
    "        except Exception as e:\n",
    "            logging.error('Error occured while in function WithOutHeaderTrailer', exc_info=True)    \n",
    "\n",
    "elif(fileformat == 'json' or fileformat == 'avro' or fileformat == 'parquet' or fileformat == 'orc'):\n",
    "    try:\n",
    "        logging.info('header_trailer_flag is N and fileformat is %s , hence calling WithOutHeaderTrailer',fileformat)\n",
    "        WithOutHeaderTrailer(attendence_id,feedname, filename, path, filetype, filedelimiter, ArrivalTimeStamp, FileSize, busdate_calender,processType,fileformat)\n",
    "    except Exception as e:\n",
    "        logging.error('Error occured while in function WithOutHeaderTrailer', exc_info=True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def funtion1(name):\n",
    "    return name+'+hi'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = funtion1('bhagya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= 'bhagya'\n",
    "a = 'file://'+path\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
