{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import os, time  #import statements\n",
    "from datetime import datetime\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from stat import *\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "startTime = datetime.now()\n",
    "\n",
    "import datetime\n",
    "ts = time.time()\n",
    "currentTimeStamp= datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "logging.basicConfig(filename='Ingestion_'+currentTimeStamp+'.log', filemode='w',format='%(asctime)s - %(message)s',datefmt='%d-%b-%y %H:%M:%S', level=logging.INFO)\n",
    "logging.info('Job Started : %s',currentTimeStamp)\n",
    "SparkContext.setSystemProperty(\"hive.metastore.uris\", \"thrift://10.3.2.20:9083\")\n",
    "sparkSession = (SparkSession.builder.appName('pyspark-to-load-tables-hive').enableHiveSupport().getOrCreate())\n",
    "spark = SparkSession.builder.appName('changeColNames').getOrCreate()\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "logging.info('User logged in and created SparkContext and SparkSession')\n",
    "\n",
    "\n",
    "feedname = sys.argv[1]\n",
    "filename = sys.argv[2]\n",
    "logging.info('Feteched command line arguments %s and %s',feedname,filename)\n",
    "try:\n",
    "    selectSql = 'SELECT * FROM etl.feedcontrol' + ' WHERE feedname' + \" ='\" + feedname + \"' AND filename\" + \" ='\" + filename +\"'\"\n",
    "    logging.info('%s',selectSql)\n",
    "    feed = sparkSession.sql(selectSql)\n",
    "    feed.show()\n",
    "    cal = sparkSession.sql('SELECT * FROM etl.calender where openindicator =\"Y\" ')  #calender table\n",
    "    cal.show()\n",
    "    filemetadata_selectSql = 'SELECT * FROM etl.filemetadata' + ' WHERE feedname' + \" ='\" + feedname + \"' AND filename\" + \" ='\" + filename +\"'\"\n",
    "    filemetadata = sparkSession.sql(filemetadata_selectSql)\n",
    "    filemetadata.show()\n",
    "    try:\n",
    "        df_ATracker = sparkSession.sql('SELECT max(attendenceid) as attendence_id FROM ETL.Attendence_Tracker order by attendence_id ')  #Attendence_tracker\n",
    "        attendenceid = df_ATracker.collect()[0]['attendence_id']\n",
    "        attendence_id = int(attendenceid) + 1\n",
    "        logging.info('value of attendence_id is %s', attendence_id)\n",
    "    except Exception as e:\n",
    "        logging.error('Error occured while creating dataframe for attendence_tarcker tables', exc_info=True)\n",
    "    feedname = feed.collect()[0]['feedname']\n",
    "    filename = feed.collect()[0]['filename']\n",
    "    landingpath = feed.collect()[0]['landingpath']\n",
    "    sourceTableName = feed.collect()[0]['sourcetablename']\n",
    "    header_trailer_flag = feed.collect()[0]['header_trailer_flag']\n",
    "    fileformat = feed.collect()[0]['fileformat']\n",
    "    rawzonepath = feed.collect()[0]['rawzonepath']\n",
    "    processType = feed.collect()[0]['processtype']\n",
    "    busdate_calender = cal.collect()[0]['busdate']\n",
    "    if(fileformat == 'DELIMITED' or fileformat == 'FIXED'):\n",
    "        filedelimiter = filemetadata.collect()[0]['filedelimiter']\n",
    "    if(fileformat == 'database'):\n",
    "        jsonpath = \"/home/developer/Vedashree/MysqlTestTable.json\"\n",
    "    if(fileformat == 'DELIMITED' or 'FIXED'):\n",
    "        path = str(\"%s/%s\" % (landingpath, filename))\n",
    "        print(filename)\n",
    "    else:\n",
    "        path = str(\"%s/%s%s%s\" % (landingpath, filename,\".\",fileformat))\n",
    "    rawzonepath = str(\"%s%s\" % (rawzonepath, filename))\n",
    "    print(rawzonepath)\n",
    "    \n",
    "    if(fileformat == 'database'):\n",
    "        ArrivalTimeStamp=currentTimeStamp\n",
    "        logging.info('ArrivalTimestamp for MysqlDB fileformat %s',ArrivalTimeStamp)\n",
    "    else:                                                                                       #extracting arrival time stamp of file from system\n",
    "        ArrivalTimeStamp = time.ctime(os.path.getctime(path))\n",
    "    if(fileformat == 'database'):  #extracting file size\n",
    "        FileSize = 0\n",
    "    else :\n",
    "        st = os.stat(path)\n",
    "        FileSize = st[ST_SIZE]\n",
    "except Exception as e:\n",
    "    logging.info('Job is FAILED')\n",
    "    logging.error('Error occured while creating dataframe or extracting data from feedcontrol and calender tables', exc_info=True)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
