{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mysqlInsert(mapperInsertSet, feedcontrolInsetSet, filemetadataInsertSet):\n",
    "    import mysql.connector\n",
    "    from mysql.connector import Error\n",
    "    from mysql.connector import errorcode\n",
    "    import pandas as pd\n",
    "\n",
    "    try:\n",
    "        connection = mysql.connector.connect(host='10.3.2.13',\n",
    "                                 database='etl',\n",
    "                                 user='etlmysql',\n",
    "                                 password='tata@123')\n",
    "        \n",
    "        mycursor = connection.cursor()\n",
    "        retVal=''\n",
    "        \n",
    "        #Mapper table inserts\n",
    "        if len(mapperInsertSet) > 0 :\n",
    "            sql_insert_query = \"\"\" INSERT INTO etl.mapper (feedname, key_col, value_col) \n",
    "                               VALUES (%s,%s,%s) \"\"\"        \n",
    "            result  = mycursor.executemany(sql_insert_query, mapperInsertSet)\n",
    "            connection.commit()\n",
    "            retVal=str(mycursor.rowcount) + \" Records inserted successfully into etl.mapper table\"\n",
    "        \n",
    "        #Mapper table inserts\n",
    "        if len(feedcontrolInsetSet) > 0 :\n",
    "            sql_insert_query = \"\"\" INSERT INTO etl.feedcontrol (feedname,filename,dataformat,landingpath,rejectthreshold,processtype,headertrailerflag,rawzonepath,sourcetablename) \n",
    "                               VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s) \"\"\"        \n",
    "            result  = mycursor.executemany(sql_insert_query, feedcontrolInsetSet)\n",
    "            connection.commit()\n",
    "            retVal=retVal + '\\n' + str(mycursor.rowcount) + \" Records inserted successfully into etl.feedcontrol table\"\n",
    "\n",
    "        #Mapper table inserts\n",
    "        if len(filemetadataInsertSet) > 0 :\n",
    "            sql_insert_query = \"\"\" INSERT INTO etl.filemetadata (feedname,filename,filetype,filedelimiter) \n",
    "                               VALUES (%s,%s,%s,%s) \"\"\"        \n",
    "            result  = mycursor.executemany(sql_insert_query, feedcontrolInsetSet)\n",
    "            connection.commit()\n",
    "            retVal=retVal + '\\n' + str(mycursor.rowcount) + \" Records inserted successfully into etl.filemetadata table\"\n",
    "            \n",
    "        return retVal\n",
    "    except mysql.connector.Error as error :\n",
    "        retVal =\"Failed inserting records into metatdata tables\" + error\n",
    "        return retVal\n",
    "    finally:\n",
    "        #closing database connection.\n",
    "        if(connection.is_connected()):\n",
    "            mycursor.close()\n",
    "            connection.close()\n",
    "            #print(\"connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Records inserted successfully into etl.mapper table\n",
      "2 Records inserted successfully into etl.filemetadata table\n"
     ]
    }
   ],
   "source": [
    "dbname='abc'\n",
    "user='abc'\n",
    "feedname='TEST'\n",
    "mapperSet = [ (feedname,'USER',user) , (feedname,'DBNAME',dbname) ]\n",
    "#feedcontrolSet = [(feedname, '', 'MYSQL', '', 0, '', 'N', '', 'abc' )]\n",
    "feedcontrolSet=[]\n",
    "filemetadataInsertSet = [('TEst3', 'abc.dat', 'DELIMITED', ',')]\n",
    "ret=mysqlInsert(mapperSet, feedcontrolSet, filemetadataInsertSet)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedname</th>\n",
       "      <th>filename</th>\n",
       "      <th>dataformat</th>\n",
       "      <th>landingpath</th>\n",
       "      <th>rejectthreshold</th>\n",
       "      <th>processtype</th>\n",
       "      <th>headertrailerflag</th>\n",
       "      <th>sourcetablename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEst3</td>\n",
       "      <td>abc.dat</td>\n",
       "      <td>DELIMITED</td>\n",
       "      <td>/home/landing</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>abc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEst3</td>\n",
       "      <td>abc.dat</td>\n",
       "      <td>DELIMITED</td>\n",
       "      <td>/home/landing</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>abc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEst3</td>\n",
       "      <td>abc.dat</td>\n",
       "      <td>DELIMITED</td>\n",
       "      <td>/home/landing</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>abc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEst3</td>\n",
       "      <td>abc.dat</td>\n",
       "      <td>DELIMITED</td>\n",
       "      <td>/home/landing</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>abc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEst3</td>\n",
       "      <td>abc.dat</td>\n",
       "      <td>DELIMITED</td>\n",
       "      <td>/home/landing</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>abc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feedname filename dataformat    landingpath  rejectthreshold processtype  \\\n",
       "0    TEst3  abc.dat  DELIMITED  /home/landing                0               \n",
       "1    TEst3  abc.dat  DELIMITED  /home/landing                0               \n",
       "2    TEst3  abc.dat  DELIMITED  /home/landing                0               \n",
       "3    TEst3  abc.dat  DELIMITED  /home/landing                0               \n",
       "4    TEst3  abc.dat  DELIMITED  /home/landing                0               \n",
       "\n",
       "  headertrailerflag sourcetablename  \n",
       "0                 N             abc  \n",
       "1                 N             abc  \n",
       "2                 N             abc  \n",
       "3                 N             abc  \n",
       "4                 N             abc  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "connection = mysql.connector.connect(host='10.3.2.13',\n",
    "                         database='etl',\n",
    "                         user='etlmysql',\n",
    "                         password='tata@123')\n",
    "\n",
    "#mycursor = connection.cursor()\n",
    "#mycursor.execute(\"SELECT * FROM etl.feedcontrol where feedname = 'TEst3'  \")\n",
    "#row_headers=[x[0] for x in mycursor.description] #this will extract row headers\n",
    "#rv = mycursor.fetchall()\n",
    "#json_data=[]\n",
    "#for result in rv:\n",
    "#    json_data.append(dict(zip(row_headers,result)))\n",
    "#return json.dumps(json_data)\n",
    "#myresult = mycursor.fetchall()\n",
    "#print(json.dumps(json_data))\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM etl.feedcontrol where feedname = 'TEst3'  \", connection)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import os, time  #import statements\n",
    "from datetime import datetime\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from stat import *\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as f\n",
    "SparkContext.setSystemProperty(\"hive.metastore.uris\", \"thrift://10.3.2.20:9083\") # comment\n",
    "sparkSession = (SparkSession.builder.appName('pyspark-to-load-tables-hive').enableHiveSupport().getOrCreate())\n",
    "spark = SparkSession.builder.appName('changeColNames').getOrCreate()\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/jars/hadoop-aws-2.7.3.jar /opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/jars/aws-java-sdk-1.7.4.jar pyspark-shell'\n",
    "\n",
    "#SparkContext.hadoopConfiguration.set(\"fs.s3n.awsAccessKeyId\", '2YQT684ZYDIDXC6K8NHP')\n",
    "#SparkContext.hadoopConfiguration.set(\"fs.s3n.awsSecretAccessKey\", '6n0BcktZJB7uVLUM9BioDsFgzD3B+K+ctwxOZije')\n",
    "#SparkContext.hadoopConfiguration.set(\"fs.s3n.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+------+----+----+\n",
      "| _c0|  _c1| _c2|   _c3| _c4| _c5|\n",
      "+----+-----+----+------+----+----+\n",
      "|   1|   KA|  IN|     4|   5|null|\n",
      "|asds|sdfdf|sddd|dsfdsf|dffd|null|\n",
      "|   3|    5|   6|  fgfg|  44|null|\n",
      "+----+-----+----+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lines = sc.textFile(\"file:///home/etl/praveen/commands.txt\")\n",
    "#lines.take(1)\n",
    "#df.write.format('csv').save(rawzonepath, mode='append')\n",
    "filename=\"file:///home/etl/test.dat\"\n",
    "colsep='|'\n",
    "hdfsFile='hdfs://10.3.2.13:8020/user/hadoop/rawzone/test.dat'\n",
    "df=spark.read.load(filename, format='csv', sep=colsep)\n",
    "df.show()\n",
    "df.write.format('csv').save(hdfsFile, mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
